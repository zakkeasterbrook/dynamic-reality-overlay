<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Advanced Object Detector</title>
    <!-- Include ONNX.js for running YOLOv5 in the browser -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <style>
        body {
            margin: 0;
            overflow: hidden;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            background: linear-gradient(45deg, #0f0f0f, #1e1e1e);
            color: #fff;
            font-family: Arial, sans-serif;
        }
        #camera {
            width: 100%;
            height: 70%;
            object-fit: cover;
            border: 2px solid #444;
            border-radius: 10px;
            position: relative;
            z-index: 1;
        }
        #visual-feedback {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 70%;
            pointer-events: none;
            z-index: 2;
        }
        #output {
            font-size: 24px;
            font-weight: bold;
            text-shadow: 0 0 5px #000;
            margin-top: 10px;
            z-index: 3;
        }
        #error-log {
            color: #f66;
            font-size: 14px;
            margin-top: 10px;
            z-index: 3;
        }
        #controls {
            position: fixed;
            bottom: 20px;
            display: flex;
            justify-content: center;
            gap: 20px;
            z-index: 4;
        }
        .button {
            padding: 10px 25px;
            font-size: 16px;
            background: #28a745;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            transition: background 0.3s, transform 0.3s;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);
        }
        .button:hover {
            background: #218838;
            transform: translateY(-2px);
        }
        .button:active {
            background: #1e7e34;
            transform: translateY(1px);
        }
    </style>
</head>
<body>
    <video id="camera" autoplay playsinline></video>
    <canvas id="visual-feedback"></canvas>
    <div id="output">Objects detected: --</div>
    <div id="error-log"></div>

    <div id="controls">
        <button class="button" onclick="resetTracking()">Reset Tracking</button>
        <button class="button" onclick="toggleBoundingBox()">Toggle Bounding Box</button>
        <button class="button" onclick="adjustSensitivity()">Adjust Sensitivity</button>
    </div>

    <script>
        const video = document.getElementById('camera');
        const canvas = document.getElementById('visual-feedback');
        const context = canvas.getContext('2d');
        const output = document.getElementById('output');
        const errorLog = document.getElementById('error-log');
        let session;
        let showBoundingBox = true;

        // Load the ONNX model for YOLOv5
        async function loadModel() {
            try {
                session = await ort.InferenceSession.create('./yolov5s.onnx');
                logError("Model loaded successfully.");
                startCamera();
            } catch (error) {
                logError(`Error loading model: ${error.message}`);
            }
        }

        // Start camera
        async function startCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: { facingMode: { ideal: 'environment' }, frameRate: { ideal: 30, max: 60 } }
                });
                video.srcObject = stream;
                video.onloadedmetadata = () => {
                    adjustCanvasSize();
                    detectObjects();
                };
                window.addEventListener('resize', adjustCanvasSize);
            } catch (error) {
                logError(`Error accessing the camera: ${error.message}. Please check camera permissions or try a different device.`);
            }
        }

        // Adjust canvas size to match video dimensions
        function adjustCanvasSize() {
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            const aspectRatio = video.videoWidth / video.videoHeight;
            const videoContainer = document.getElementById('camera');
            videoContainer.style.width = `${window.innerWidth}px`;
            videoContainer.style.height = `${window.innerWidth / aspectRatio}px`;
        }

        // Detect objects using the YOLOv5 model
        async function detectObjects() {
            try {
                // Prepare the image tensor for input
                const tensor = await preprocessImage(video, canvas.width, canvas.height);
                const feeds = { 'images': tensor };
                const results = await session.run(feeds);
                const boxes = results.boxes.data;
                const scores = results.scores.data;
                const labels = results.labels.data;

                context.clearRect(0, 0, canvas.width, canvas.height);
                drawResults(boxes, scores, labels);
                requestAnimationFrame(detectObjects);
            } catch (error) {
                logError(`Error during detection: ${error.message}`);
            }
        }

        // Preprocess the image from the video feed for the model
        async function preprocessImage(video, width, height) {
            context.drawImage(video, 0, 0, width, height);
            const imageData = context.getImageData(0, 0, width, height);
            const data = imageData.data;
            // Normalize the pixel values
            const float32Data = new Float32Array(width * height * 3);
            for (let i = 0; i < data.length; i += 4) {
                float32Data[i / 4 * 3] = data[i] / 255.0;
                float32Data[i / 4 * 3 + 1] = data[i + 1] / 255.0;
                float32Data[i / 4 * 3 + 2] = data[i + 2] / 255.0;
            }
            return new ort.Tensor('float32', float32Data, [1, 3, height, width]);
        }

        // Draw detection results
        function drawResults(boxes, scores, labels) {
            const threshold = 0.5; // Confidence threshold
            for (let i = 0; i < scores.length; i++) {
                if (scores[i] > threshold) {
                    const [x, y, width, height] = boxes.slice(i * 4, i * 4 + 4).map(v => v * canvas.width);
                    context.strokeStyle = 'cyan';
                    context.lineWidth = 2;
                    context.strokeRect(x, y, width, height);
                    context.fillStyle = 'cyan';
                    context.font = '16px Arial';
                    context.fillText(
                        `${labels[i]} - ${Math.round(scores[i] * 100)}%`,
                        x,
                        y > 10 ? y - 5 : 10
                    );
                }
            }
            output.textContent = `Objects detected: ${labels.length}`;
        }

        // Log errors
        function logError(message) {
            errorLog.textContent += `${message}\n`;
            console.error(message);
        }

        // Reset tracking data
        function resetTracking() {
            output.textContent = "Objects detected: --";
            logError("Tracking reset.");
        }

        // Toggle the bounding box visibility
        function toggleBoundingBox() {
            showBoundingBox = !showBoundingBox;
            logError(`Bounding box ${showBoundingBox ? 'enabled' : 'disabled'}.`);
        }

        // Adjust sensitivity of detection
        function adjustSensitivity() {
            const newThreshold = prompt("Set detection threshold (default 0.5):", 0.5);
            logError(`Detection threshold adjusted to ${newThreshold}.`);
        }

        // Load model and start the application
        loadModel();
    </script>
</body>
</html>
